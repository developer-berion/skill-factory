<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# KB_16_Data_Incidents_Playbook.md

Prompt:
‚ÄúInvestiga y redacta un playbook de incidentes de datos para CRM (2025‚Äì2026): detecci√≥n, triage, comunicaci√≥n (banners, status page), degradaci√≥n controlada, escalamiento, postmortem, y c√≥mo documentarlo para ventas (sin sonar a excusa).
Incluye:
SLI/SLO definiciones claras para data freshness, completeness, correctness
Plantilla de postmortem y checklist de ‚Äòcustomer comms‚Äô
Patrones de ‚Äògraceful degradation‚Äô para dashboards y forecast
Citas por secci√≥n. Facts vs Inferences.‚Äù

---

# KB_16 ‚Äî Data Incidents Playbook for CRM (2025‚Äì2026)


***

## Executive Summary

Un incidente de datos en CRM es cualquier degradaci√≥n no planificada de la calidad, disponibilidad o confiabilidad de los datos que impacta decisiones de negocio: pipelines de ventas contaminados, forecasts sesgados, registros incompletos que llegan a equipos de campo. En contextos B2B enterprise este tipo de incidentes son m√°s peligrosos que una ca√≠da de servicio porque son invisibles: el sistema "funciona" pero entrega informaci√≥n falsa.[^1]

Este playbook cubre el ciclo completo: detecci√≥n temprana v√≠a SLI/SLO, triage por severidad, comunicaci√≥n estructurada (banners, status pages), degradaci√≥n controlada para que ventas no quede ciega, escalamiento, postmortem blameless y documentaci√≥n para audiencias comerciales (sin sonar a excusa). Aplica tanto a CRM cloud (Salesforce, HubSpot) como a pipelines internos de datos. Se alinea con pr√°cticas de data contracts y engineering confiable en producci√≥n 2025‚Äì2026.[^2][^3]

**Alcance:** CRM enterprise + pipelines de datos de ventas.
**Audiencia primaria:** RevOps, Data Engineering, Sales Leadership, Customer Success.
**Nivel de madurez m√≠nimo recomendado:** pipelines con dbt tests o equivalente + owner definido por dataset.

***

## Definitions and Why It Matters

**`FACT`** Un **SLI (Service Level Indicator)** es la m√©trica real que se mide: por ejemplo, `% de registros con customer_id no nulo` o `lag entre evento y disponibilidad en CRM`.[^2]

**`FACT`** Un **SLO (Service Level Objective)** es el target acordado sobre ese SLI: `‚â• 99% completeness en campos cr√≠ticos` o `datos disponibles en < 4 horas desde evento`. Es un compromiso interno, no contractual.[^3]

**`FACT`** Un **SLA (Service Level Agreement)** es el acuerdo formal externo (con penalidades). Para datos internos de CRM, raramente existe un SLA; los SLOs son la herramienta de gobierno correcta.[^3]

### SLIs/SLOs por dimensi√≥n de calidad de datos CRM

| Dimensi√≥n | SLI (m√©trica) | SLO target sugerido | Impacto si falla |
| :-- | :-- | :-- | :-- |
| **Freshness** | Lag entre evento de venta y registro en CRM | < 4 h en batch; < 15 min en CDC | Forecast sesgado, reporte diario in√∫til [^3] |
| **Completeness** | % de campos cr√≠ticos no nulos (`company_domain`, `deal_stage`, `owner_id`) | ‚â• 99.5% en Tier-1 | Lead scoring ciego, territorios incorrectos [^2] |
| **Correctness (Accuracy)** | % registros que pasan validaciones de reglas de negocio (`price > 0`, `stage ‚àà set`) | ‚â• 99% en tier-1 | Contratos mal emitidos, comisiones err√≥neas [^4] |
| **Uniqueness** | Tasa de duplicados por clave primaria o compuesta | < 0.5% duplicates en deals activos | Pipeline inflado, doble contacto a clientes [^2] |
| **Consistency** | Integridad referencial entre objetos (deal ‚Üí account ‚Üí contact) | 100% en relaciones cr√≠ticas | CRM inoperable para actividad diaria [^4] |

**`INFERENCE`** En LATAM, donde la calidad de datos de entrada es m√°s variable (velocidad de actualizaci√≥n manual, integraciones inestables), los SLOs de completeness tienen mayor impacto real que los de freshness en la mayor√≠a de los contextos SMB mayoristas.

***

## Principles and Best Practices

### 1. Detecci√≥n: Monitoreo proactivo de SLIs

**`FACT`** Los incidentes de datos deben detectarse antes de que el negocio los reporte. Las se√±ales de alerta temprana incluyen: spike en tasa de duplicados, campos cr√≠ticos vac√≠os en ingestas recientes, o lag an√≥malo en pipelines de sincronizaci√≥n.[^1]

**`FACT`** Las herramientas recomendadas para monitoreo de data quality en producci√≥n (2025) son: **dbt tests** (validaciones en transformaci√≥n), **Monte Carlo / Bigeye** (data observability), **Datadog** para latencia de pipelines, y **Salesforce Shield** para auditor√≠a interna.[^5]

**`INFERENCE`** Para operaciones sin presupuesto de herramientas enterprise, un dashboard de dbt Cloud + alertas de Slack sobre tests fallidos cubre el 80% de la detecci√≥n necesaria.

**Se√±ales de alerta por nivel:**

- **Critical:** Completeness cae debajo de SLO en dataset Tier-1; pipeline de sincronizaci√≥n CRM sin refresh en > 2√ó ventana esperada
- **Warning:** Tasa de duplicados > 1%; lag de freshness entre 1√ó y 2√ó ventana esperada
- **Info:** Campos no-cr√≠ticos con degradaci√≥n; cambios de schema sin consumer sign-off

***

### 2. Triage: Clasificaci√≥n de Severidad

**`FACT`** La clasificaci√≥n de severidad debe estar definida antes del incidente, no durante. La ambig√ºedad en severidad es la causa principal de escalamientos innecesarios y comunicaci√≥n tard√≠a.[^6]


| Severidad | Criterio | Tiempo para ACK | Owner |
| :-- | :-- | :-- | :-- |
| **SEV-1** | Datos incorrectos llegando a forecasts de ventas o contratos activos | 15 min | Data Lead + Sales VP |
| **SEV-2** | Completeness/freshness fuera de SLO en dataset Tier-1 | 30 min | Data Engineer on-call |
| **SEV-3** | Degradaci√≥n en dataset Tier-2, sin impacto en decisiones activas | 4 h | Data Engineer |
| **SEV-4** | Inconsistencias menores, duplicados en registros hist√≥ricos | Pr√≥ximo sprint | RevOps |

**`FACT`** El primer update p√∫blico (status page o banner) debe salir dentro de los primeros 15 minutos tras declarar SEV-1 o SEV-2.[^7]

***

### 3. Comunicaci√≥n: Banners, Status Pages y Templates

**`FACT`** La comunicaci√≥n de incidentes debe separar el canal interno (war room, Slack) del canal externo (status page, banner en CRM), y asignar un Communication Lead dedicado para evitar mensajes contradictorios.[^8]

**`FACT`** El status page debe actualizarse cada 30 minutos durante un incidente activo, aunque no haya novedades, para mantener credibilidad.[^7]

#### Templates de comunicaci√≥n por etapa

**üî¥ Banner inicial en CRM (< 15 min):**

```
‚ö†Ô∏è [CRM DATA ALERT] Estamos investigando una degradaci√≥n en [Dataset/M√≥dulo].
Los datos de [Pipeline/Forecast/Contacts] pueden estar incompletos o desactualizados.
Pr√≥xima actualizaci√≥n: [HH:MM]. Incidente #[ID].
```

**üü° Update intermedio (cada 30 min):**

```
üìä [UPDATE #N ‚Äî Incidente #ID] 
Estado: En mitigaci√≥n.
Impacto confirmado: [descripci√≥n sin jerga t√©cnica].
Datos afectados: [objetos/campos espec√≠ficos].
ETA resoluci√≥n: [HH:MM o "TBD"].
Workaround disponible: [s√≠/no + instrucci√≥n].
```

**üü¢ Resoluci√≥n:**

```
‚úÖ [RESUELTO ‚Äî Incidente #ID]
Los datos de [m√≥dulo] han sido restaurados y validados.
Per√≠odo afectado: [timestamp inicio] ‚Üí [timestamp fin].
Acci√≥n requerida por usuarios: [ninguna / re-exportar reporte X].
Postmortem disponible en: [link ‚Äî 48-72h].
```

**`FACT`** El lenguaje en comunicaciones externas debe evitar t√©rminos t√©cnicos (`pipeline failure`, `schema drift`) y reemplazarlos con impacto en negocio: "los reportes de pipeline pueden mostrar datos de hasta 6 horas atr√°s".[^8][^7]

***

### 4. Graceful Degradation para Dashboards y Forecast

**`FACT`** Graceful degradation en sistemas de datos significa que cuando un dataset falla, el sistema contin√∫a funcionando con datos parciales o cacheados, en lugar de mostrar errores o datos vac√≠os silenciosamente.[^9][^5]

**`INFERENCE`** La degradaci√≥n silenciosa (dashboard que muestra datos stale sin advertencia) es m√°s peligrosa que un error expl√≠cito, porque los usuarios toman decisiones sobre informaci√≥n incorrecta sin saberlo.

#### Patrones de degradaci√≥n controlada por tipo de componente CRM

**Dashboard de Pipeline:**

- **Modo normal:** datos frescos < 4h, m√©tricas en tiempo real
- **Modo degradado:** banner visible con timestamp de √∫ltima actualizaci√≥n v√°lida + datos cacheados desde √∫ltimo estado conocido bueno
- **Modo fallback:** mostrar solo datos de ayer (batch D-1) con etiqueta clara `[DATOS AL: YYYY-MM-DD HH:MM]`

**Forecast / Revenue Prediction:**

- **Modo normal:** modelo corriendo con datos actualizados
- **Modo degradado:** congelar el forecast de la √∫ltima corrida exitosa + notificar a Sales Leadership
- **Modo fallback:** mostrar forecast manual (input humano) con bloqueo del modelo autom√°tico hasta resoluci√≥n

**Lead Scoring:**

- **Modo normal:** scoring en tiempo real
- **Modo degradado:** usar score de la √∫ltima ejecuci√≥n batch + marcar scores como `[Estimado - actualizaci√≥n pendiente]`
- **Modo fallback:** deshabilitar score autom√°tico, activar scoring manual por tier (A/B/C basado en reglas simples)

**`FACT`** La degradaci√≥n debe ser expl√≠citamente documentada en el data contract del dataset, incluyendo qu√© fallback aplica y cu√°l es el SLO de degradaci√≥n aceptable.[^2]

***

### 5. Escalamiento

**`FACT`** El √°rbol de escalamiento debe definirse en el runbook antes del incidente. La ausencia de este √°rbol genera los mayores retrasos en resoluci√≥n.[^2]

```
Nivel 1: Data Engineer on-call (0-30 min)
    ‚Üì si sin resoluci√≥n
Nivel 2: Data Lead / RevOps Manager (30-60 min)
    ‚Üì si impacto en revenue o reportes ejecutivos
Nivel 3: CTO / VP Sales / VP Customer Success (60-90 min)
    ‚Üì si impacto en clientes externos o SLA contractual
Nivel 4: Legal / Compliance / PR (si hay datos de clientes comprometidos)
```

**`INFERENCE`** En equipos < 20 personas, niveles 1 y 2 pueden ser la misma persona. Lo cr√≠tico es que el √°rbol est√© documentado y que el escalamiento a Nivel 3 no requiera consenso: debe ser una decisi√≥n unilateral del Nivel 2.

***

### 6. Postmortem: Plantilla Completa

**`FACT`** Los postmortems efectivos son blameless, se realizan dentro de 48-72 horas tras el cierre del incidente, y producen action items espec√≠ficos con owners y fechas.[^10][^6]

***

#### üìÑ PLANTILLA DE POSTMORTEM ‚Äî Incidente de Datos CRM

```markdown
## Postmortem: [T√≠tulo descriptivo del incidente]
**ID:** INC-YYYY-NNN  
**Fecha del incidente:** YYYY-MM-DD  
**Fecha del postmortem:** YYYY-MM-DD  
**Severidad:** SEV-[1/2/3]  
**Duraci√≥n:** HH:MM (detecci√≥n ‚Üí resoluci√≥n)  
**Facilitador:** [Nombre]  
**Participantes:** [Lista]

---

### 1. Resumen ejecutivo (5 l√≠neas m√°x.)
[Qu√© pas√≥, cu√°nto tiempo dur√≥, qu√© impact√≥ en negocio, c√≥mo se resolvi√≥.]

### 2. Timeline
| Timestamp | Evento | Actor |
|-----------|--------|-------|
| HH:MM | Primera se√±al detectada (alerta / reporte manual) | [Sistema/Persona] |
| HH:MM | Incidente declarado | [Nombre] |
| HH:MM | Impacto confirmado y cuantificado | [Nombre] |
| HH:MM | Mitigaci√≥n aplicada | [Nombre] |
| HH:MM | Resoluci√≥n completa | [Nombre] |
| HH:MM | Validaci√≥n de datos restaurados | [Nombre] |

### 3. Impacto
- **Datos afectados:** [Datasets, objetos, campos]
- **Usuarios impactados:** [N√∫mero / equipos]
- **Decisiones de negocio en riesgo:** [Pipeline, forecast, campa√±as, etc.]
- **SLO breach:** [S√≠/No] ‚Äî [Cu√°l SLO y por cu√°nto tiempo]
- **Impacto estimado en revenue/operaciones:** [Cuantificar si posible]

### 4. Causa ra√≠z (RCA)
**Causa inmediata:** [Lo que dispar√≥ el incidente]  
**Causa ra√≠z:** [Por qu√© el sistema/proceso permiti√≥ que ocurriera]  
**Causas contribuyentes:** [Factores que lo agravaron]

### 5. Qu√© funcion√≥ bien
- [Item 1]
- [Item 2]

### 6. Qu√© mejorar
- [Item 1]
- [Item 2]

### 7. Action items
| Acci√≥n | Owner | Fecha l√≠mite | Prioridad |
|--------|-------|--------------|-----------|
| [Acci√≥n espec√≠fica y medible] | [Nombre] | YYYY-MM-DD | Alta/Media/Baja |

### 8. Lessons learned
[2-3 p√°rrafos sobre patrones identificados y cambios sist√©micos recomendados.]
```


***

### 7. Documentaci√≥n para Ventas (Sin Sonar a Excusa)

**`FACT`** La comunicaci√≥n post-incidente hacia equipos de ventas y clientes debe focalizarse en: qu√© datos son confiables ahora, qu√© acci√≥n tomar, y qu√© se est√° haciendo para que no vuelva a ocurrir. No en detalles t√©cnicos ni en justificaciones de por qu√© pas√≥.[^6][^8]

**`INFERENCE`** En contextos B2B donde la agencia es el cliente (no el pasajero final), el mensaje comercial m√°s efectivo post-incidente es aquel que demuestra control del proceso, no que minimiza el problema.

#### Framework de comunicaci√≥n hacia ventas: 3-ACT

**ACT 1 ‚Äî Lo que pas√≥ (1 p√°rrafo, sin jerga):**
> "Entre [hora] y [hora] del [fecha], los datos de [pipeline/forecast/contactos] mostraron informaci√≥n desactualizada. Esto pudo haber afectado reportes de cierre y scores de oportunidades durante ese per√≠odo."

**ACT 2 ‚Äî Lo que es confiable ahora:**
> "A partir de [timestamp], todos los registros han sido validados. Si exportaste reportes entre [hora inicio] y [hora fin], te recomendamos regenerarlos. Los datos anteriores a [fecha referencia] no fueron afectados."

**ACT 3 ‚Äî Lo que cambia:**
> "Implementamos [medida concreta] para detectar este tipo de situaci√≥n en < [tiempo]. El pr√≥ximo [per√≠odo] recibir√°s un update sobre el avance."

***

## Examples: Aplicado a CRM Enterprise

### Caso 1: Pipeline inflado por duplicados (SEV-2)

**`FACT`** HubSpot reporta que cerca del 25% de los datos de contactos empresariales se vuelven obsoletos o duplicados cada a√±o.[^4]

**Escenario:** Pipeline report muestra \$2.3M en oportunidades; el 18% son deals duplicados generados por una integraci√≥n mal configurada entre CRM y herramienta de outreach.

**Respuesta:**

1. Detecci√≥n via dbt test de uniqueness (alert Slack a las 07:14)
2. SEV-2 declarado a las 07:30. Dashboard de pipeline congelado con banner
3. Engineering ejecuta deduplicaci√≥n en staging y valida contra prod
4. Datos restaurados a las 11:00. Pipeline real: \$1.89M
5. Comunicaci√≥n a Sales Leadership: "El pipeline real validado es \$1.89M. Hemos corregido la integraci√≥n con [tool]. El forecast de cierre mensual no se ve afectado."

### Caso 2: Freshness SLO breach en datos de forecast (SEV-1)

**Escenario:** Modelo de forecast no recibe datos de actividad de los √∫ltimos 6h por falla en job de sincronizaci√≥n. Sales VP presenta board deck con n√∫meros desactualizados.

**Respuesta graceful degradation:**

1. Forecast congelado a la √∫ltima corrida exitosa con banner `[FORECAST AL: YYYY-MM-DD 02:00]`
2. Notificaci√≥n inmediata a Sales VP antes de presentaci√≥n
3. Sales VP usa nota en deck: "Forecast validado a corte de ayer ‚Äî datos de hoy se incorporan post-reuni√≥n"
4. Restauraci√≥n en 2.5h. Forecast re-corrido y distribuido con timestamp actualizado

**`INFERENCE`** Congelar y etiquetar es siempre mejor que presentar datos stale sin advertencia, incluso si la conversaci√≥n con el board es inc√≥moda.

***

## Metrics / Success Signals

**`FACT`** Los KPIs primarios para medir la salud del programa de gesti√≥n de incidentes de datos son: tasa de incidentes por mes, MTTD (Mean Time to Detect), MTTR (Mean Time to Resolve), y porcentaje de cumplimiento de SLOs.[^3][^2]


| M√©trica | Definici√≥n | Target saludable |
| :-- | :-- | :-- |
| **MTTD** | Tiempo desde que el incidente ocurre hasta que es detectado | < 30 min (SEV-1/2) |
| **MTTR** | Tiempo desde detecci√≥n hasta resoluci√≥n completa | < 4h (SEV-1), < 24h (SEV-2) |
| **SLO Compliance %** | % de tiempo en que cada SLO se cumple | ‚â• 99% en Tier-1 |
| **Error Budget Burn Rate** | Velocidad a la que se consume el error budget del SLO | Alert si > 2√ó rate normal |
| **Postmortem completion rate** | % de SEV-1/2 con postmortem completado en 72h | 100% |
| **Action item close rate** | % de action items de postmortem cerrados en fecha acordada | ‚â• 80% en 30 d√≠as |
| **Incident recurrence rate** | % de incidentes con misma causa ra√≠z en 90 d√≠as | < 10% |

**`FACT`** El error budget es el complemento del SLO: si el SLO es 99.5% completeness, el error budget es 0.5% de tiempo/registros donde se permite falla. Cuando el error budget se consume, se frena feature development y se prioriza confiabilidad.[^3]

***

## Operational Checklist

### ‚úÖ Pre-incidente (Setup)

- [ ] SLIs definidos por dataset con owner asignado[^2]
- [ ] SLOs documentados en data contract por Tier (1/2/3)
- [ ] Alertas configuradas (dbt tests + monitoring tool) con umbral por severidad
- [ ] √Årbol de escalamiento documentado y distribuido
- [ ] Templates de comunicaci√≥n (banner, status page, email) pre-aprobados
- [ ] Runbook de fallback por componente (dashboard, forecast, scoring) documentado
- [ ] Status page o canal de comunicaci√≥n de incidentes configurado


### ‚úÖ Durante el incidente

- [ ] Incidente declarado con ID √∫nico y severidad asignada
- [ ] Communication Lead designado (persona ‚â† quien est√° resolviendo)
- [ ] Primera comunicaci√≥n externa < 15 min tras declaraci√≥n[^7]
- [ ] War room abierto (Slack/Meet) con canal separado del canal de resoluci√≥n
- [ ] Updates al status page cada 30 min[^8]
- [ ] Graceful degradation activado en componentes afectados
- [ ] Timeline documentado en tiempo real
- [ ] Escalamiento ejecutado si sin resoluci√≥n en ventana definida


### ‚úÖ Post-incidente

- [ ] Resoluci√≥n validada con datos (no solo "parece funcionar")
- [ ] Comunicaci√≥n de resoluci√≥n distribuida a todos los canales[^7]
- [ ] Usuarios notificados sobre acci√≥n requerida (re-exportar reportes, etc.)
- [ ] Postmortem agendado dentro de 48h
- [ ] Postmortem completado y distribuido dentro de 72h[^6]
- [ ] Action items creados en sistema de tracking (Jira, Linear, etc.)
- [ ] SOURCES.md / base de conocimiento actualizada
- [ ] SLO compliance report actualizado

***

## Anti-Patterns

**`FACT`** Los anti-patrones m√°s comunes en gesti√≥n de incidentes de datos enterprise que deben evitarse activamente:[^10][^1][^6]

1. **Degradaci√≥n silenciosa:** Dashboards que muestran datos stale sin banner ni timestamp visible. Peor caso posible porque los usuarios no saben que est√°n ciegos.
2. **Blame game en postmortem:** Postmortems que terminan identificando a una persona como causa ra√≠z. Destruyen la cultura de transparencia necesaria para que los incidentes futuros sean reportados r√°pido.
3. **"Ya estamos investigando" sin fecha de update:** Primera comunicaci√≥n que no incluye ETA del pr√≥ximo update. Genera silencio ansiog√©nico y escalamientos innecesarios.
4. **SLOs sin owner:** SLOs definidos en papel pero sin equipo responsable de su cumplimiento. El error budget se consume sin que nadie lo defienda.
5. **Postmortem sin action items medibles:** Documentos que concluyen con "mejorar monitoreo" sin especificar qu√©, qui√©n y cu√°ndo. Son archivos muertos.
6. **Comunicar a ventas en lenguaje t√©cnico:** "El pipeline ETL fall√≥ por un schema drift en la tabla de oportunidades" no le dice a un vendedor qu√© hacer ahora.
7. **Resolver sin validar:** Cerrar el incidente porque el job volvi√≥ a correr sin verificar que los datos downstream son correctos. El incidente "resuelto" puede estar produciendo datos incorrectos.
8. **`INFERENCE`** En equipos de turismo/B2B con ciclos de ventas cortos, el anti-patr√≥n m√°s costoso es el \#1: un forecast inflado por datos stale puede generar compromisos de cupo con proveedores incorrectos.

***

## Diagnostic Questions

Estas preguntas identifican el nivel de madurez en gesti√≥n de incidentes de datos y los gaps prioritarios:

**Sobre detecci√≥n:**

- ¬øTienes alertas autom√°ticas que detectan cuando un dataset no se actualiza en el tiempo esperado?
- ¬øCu√°ntos incidentes de datos en los √∫ltimos 6 meses fueron reportados por el equipo t√©cnico vs. por ventas?

**Sobre SLOs:**

- ¬øPuedes decir hoy cu√°l es el SLO de freshness para el dataset de pipeline de oportunidades?
- ¬øExiste un owner nominado para cada dataset Tier-1 que sea responsable del SLO?

**Sobre comunicaci√≥n:**

- ¬øTienes templates pre-aprobados para comunicar incidentes, o se redactan desde cero cada vez?
- ¬øLos usuarios de CRM saben a d√≥nde ir para ver el estado de los datos durante un incidente?

**Sobre degradaci√≥n:**

- ¬øQu√© pasa en tu CRM cuando el pipeline de datos falla? ¬øLos dashboards muestran error, datos stale, o nada?
- ¬øTienes un fallback documentado para el caso de que el forecast autom√°tico no est√© disponible?

**Sobre postmortem:**

- ¬øCu√°ntos de los incidentes del √∫ltimo a√±o tienen un postmortem escrito?
- ¬øLos action items de postmortems anteriores est√°n siendo cerrados o viven en documentos que nadie revisita?

***

## Key Takeaways for PM Practice

- **`FACT`** Define SLOs antes de necesitarlos: un SLO sin due√±o es decoraci√≥n. Cada dataset Tier-1 necesita owner, target, y error budget documentado.[^2]
- **`FACT`** La primera comunicaci√≥n importa m√°s que la velocidad de resoluci√≥n: un update claro en 15 min construye m√°s confianza que resolver en silencio en 2 horas.[^7]
- **`INFERENCE`** En CRM B2B, el costo real de un incidente de datos no es t√©cnico: es el forecast incorrecto que lleva a una promesa de revenue que no existe.
- **`FACT`** Graceful degradation debe dise√±arse antes del incidente: definir expl√≠citamente qu√© muestra cada componente cuando sus datos fallan.[^5][^9]
- **`FACT`** Los postmortems blameless son la √∫nica forma sostenible de crear una cultura donde los incidentes se reportan r√°pido en lugar de ocultarse.[^10][^6]
- **`INFERENCE`** Para equipos en LATAM con fricci√≥n operativa, un sistema simple (dbt tests + Slack alerts + runbook en Notion) implementado consistentemente supera cualquier herramienta enterprise mal configurada.
- Documentar para ventas significa traducir: de "schema drift en tabla de deals" a "el reporte de cierre de este mes puede tener datos de hasta ayer".
- El postmortem no es la fase final: el seguimiento de action items a 30/60/90 d√≠as es donde realmente se construye resiliencia.

***

## Sources

| ID | Fuente | Fecha | Relevancia |
| :-- | :-- | :-- | :-- |
| [^2] | Petronella Tech ‚Äî Data Contracts for AI/CRM SLIs/SLOs | Nov 2025 | SLI/SLO framework, completeness/freshness targets |
| [^3] | Uptrace ‚Äî SLA/SLO Monitoring Requirements | Jun 2025 | Error budget, freshness SLOs, incident dashboards |
| [^6] | OneUptime ‚Äî Incident Postmortem Templates | Sep 2025 | Postmortem structure, blameless principles |
| [^1] | LinkedIn/Erasala ‚Äî AI Integrity Early Warning Signals | Feb 2026 | Degradaci√≥n silenciosa, se√±ales de alerta |
| [^10] | iLert ‚Äî Postmortem Template | Mar 2025 | RCA, impact assessment, lessons learned |
| [^5] | SuperAGI ‚Äî Self-Healing AI Agents 2025 | Jun 2025 | Graceful degradation, chaos engineering, MTTR |
| [^9] | Informatica ‚Äî Enterprise AI Agent Engineering | Dec 2025 | Fallback scenarios, graceful degradation patterns |
| [^8] | UptimeRobot ‚Äî Status Page Ultimate Guide 2026 | Ene 2026 | Status page best practices, incident communication |
| [^7] | OneUptime ‚Äî Incident Communication Templates | Ene 2026 | Templates, timing, communication lead role |
| [^4] | Revenue Grid ‚Äî CRM Data Quality | Dic 2025 | Dimensiones de calidad (accuracy, completeness, timeliness) |

> üìé **SOURCES.md:** Agregar entradas anteriores a `SOURCES.md` del Space bajo la categor√≠a `Data Quality & Incident Management`. Verificar duplicados contra KB_01‚ÄìKB_15 antes de insertar.
<span style="display:none">[^11][^12][^13][^14][^15][^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^26][^27][^28][^29][^30][^31]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://www.linkedin.com/posts/naveen-erasala_the-early-warning-signals-that-ai-integrity-activity-7425902122707349505--gTc

[^2]: https://petronellatech.com/blog/managed-services/data-contracts-the-new-sla-for-reliable-ai-analytics-crm/

[^3]: https://uptrace.dev/blog/sla-slo-monitoring-requirements

[^4]: https://revenuegrid.com/blog/crm-data-quality/

[^5]: https://superagi.com/mastering-self-healing-ai-agents-in-2025-a-beginners-guide-to-detection-prevention-and-correction/

[^6]: https://oneuptime.com/blog/post/2025-09-09-effective-incident-postmortem-templates-ready-to-use-examples/view

[^7]: https://oneuptime.com/blog/post/2026-01-30-incident-communication-templates/view

[^8]: https://uptimerobot.com/knowledge-hub/monitoring/building-a-status-page-ultimate-guide/

[^9]: https://www.informatica.com/resources/articles/enterprise-ai-agent-engineering.html

[^10]: https://www.ilert.com/blog/postmortem-template-to-optimize-your-incident-response

[^11]: pasted-text.txt

[^12]: https://growintandem.com/crm-audit-playbook/

[^13]: https://www.revenuetools.io/blog/crm-data-hygiene

[^14]: https://www.vbeyonddigital.com/blog/global-capability-centers-as-an-ai-value-factory-operating-model-outcome-slas-not-effort-for-analytics-automation-and-cloud-ops/

[^15]: https://www.articsledge.com/post/ai-demand-forecasting-seasonal-sales-businesses

[^16]: https://www.cloudrangecyber.com/news/incident-response-playbook-testing-guide-2025-reduce-breach-costs-by-50

[^17]: https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/gen-ai-lifecycle-operational-excellence.pdf

[^18]: https://uptimerobot.com/knowledge-hub/monitoring/ultimate-post-mortem-templates/

[^19]: https://www.itsm-docs.com/blogs/it-operations-playbook/top-tools-for-incident-management-2025

[^20]: https://www.atlassian.com/incident-management/incident-communication/templates

[^21]: https://www.supportbench.com/customer-facing-status-page-incident-comms-playbook/

[^22]: https://incident.io/changelog/status-page-templates-to-simplify-incident-communication

[^23]: https://support.freshservice.com/support/solutions/articles/50000009352-publishing-an-incident-to-the-status-page

[^24]: https://www.cleanlist.ai/blog/2026-02-20-b2b-data-enrichment-complete-guide

[^25]: https://www.artisan.co/blog/crm-data-management

[^26]: https://runframe.io/blog/incident-stakeholder-communication-templates

[^27]: https://lagrowthmachine.com/crm-system-examples-2026/

[^28]: https://up.report/blog/ranking-top-status-pages-2025

[^29]: https://ohdear.app/news-and-updates/new-feature-status-page-update-templates

[^30]: https://oneuptime.com/blog/post/2026-01-30-freshness-slos/view

[^31]: https://instatus.com/blog/outage-notification-templates

